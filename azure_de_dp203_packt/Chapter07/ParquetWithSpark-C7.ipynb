{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4b960fbf-0f02-4bc8-b075-9354a420f049","showTitle":false,"title":""}},"source":["Use the following Azure Databricks storage setup block only if you are using Azure Databricks. You can refer to the instructions here to get started:\n","https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sp-access\n","\n","If you are using Synapse Spark and if your data is residing on the storage attached to the Synapse Spark workspace, you can skip the below storage setup section."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b7e17651-5f7d-4f4b-bd97-a3b0079f5770","showTitle":false,"title":""}},"outputs":[],"source":["%scala\n","\n","val storageAccountName = \"<INSERT STORAGE ACCOUNT>\"\n","val fileSystemName = \"<INSERT CONTAINER NAME>\"\n","\n","val commonPath = \"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net\"\n","\n","# AAD Application Details\n","val appID = \"<INSERT APP ID>\"\n","val secret = \"<INSERT SECRET>\"\n","val tenantID = \"<INSERT TENANT ID>\"\n","\n","spark.conf.set(\"fs.azure.account.auth.type.\" + storageAccountName + \".dfs.core.windows.net\", \"OAuth\")\n","spark.conf.set(\"fs.azure.account.oauth.provider.type.\" + storageAccountName + \".dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.id.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + appID + \"\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.secret.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + secret + \"\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.\" + storageAccountName + \".dfs.core.windows.net\", \"https://login.microsoftonline.com/\" + tenantID + \"/oauth2/token\")\n","spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"true\")\n","dbutils.fs.ls(\"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net/\")\n","spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2b265bb0-668b-4299-813a-85a0ce9f4ac9","showTitle":false,"title":""}},"outputs":[],"source":["%scala\n","\n","// Let us generate some parquet data first\n","import org.apache.spark.sql.{DataFrame, Row, SaveMode}\n","import org.apache.spark.sql.types.{StringType, IntegerType, StructField, StructType}\n","\n","val tripsParquetPath = commonPath + \"/parquet/trips\"\n","val driverParquetPath = commonPath + \"/parquet/driver\"\n","\n","// Generate sample trips data\n","val tripSchema = new StructType().add(\"tripId\", StringType).add(\"driverId\", StringType).add(\"customerId\", StringType).add(\"cabId\", StringType).add(\"tripDate\", StringType).add(\"startLocation\", StringType).add(\"endLocation\", StringType)\n","\n","val tripData = Seq(\n","  Row(\"100\", \"200\", \"300\", \"400\", \"20220101\", \"New York\", \"New Jersey\"),\n","  Row(\"101\", \"201\", \"301\", \"401\", \"20220102\", \"Tempe\", \"Phoenix\"),\n","  Row(\"102\", \"202\", \"302\", \"402\", \"20220103\", \"San Jose\", \"San Franciso\"),\n","  Row(\"103\", \"203\", \"303\", \"403\", \"20220102\", \"New York\", \"Boston\"),\n","  Row(\"104\", \"204\", \"304\", \"404\", \"20220103\", \"New York\", \"Washington\"),\n","  Row(\"105\", \"205\", \"305\", \"405\", \"20220201\", \"Miami\", \"Fort Lauderdale\"),\n","  Row(\"106\", \"206\", \"306\", \"406\", \"20220202\", \"Seattle\", \"Redmond\"),\n","  Row(\"107\", \"207\", \"307\", \"407\", \"20220203\", \"Los Angeles\", \"San Diego\"),\n","  Row(\"108\", \"208\", \"308\", \"408\", \"20220301\", \"Phoenix\", \"Las Vegas\"),\n","  Row(\"109\", \"209\", \"309\", \"409\", \"20220302\", \"Washington\", \"Baltimore\"),\n","  Row(\"110\", \"210\", \"310\", \"410\", \"20220303\", \"Dallas\", \"Austin\"),\n","  Row(\"111\", \"211\", \"311\", \"411\", \"20220303\", \"New York\", \"New Jersey\"),\n","  Row(\"112\", \"212\", \"312\", \"412\", \"20220304\", \"New York\", \"Boston\"),\n","  Row(\"113\", \"212\", \"312\", \"412\", \"20220401\", \"San Jose\", \"San Ramon\"),\n","  Row(\"114\", \"212\", \"312\", \"412\", \"20220404\", \"San Jose\", \"Oakland\"),\n","  Row(\"115\", \"212\", \"312\", \"412\", \"20220404\", \"Tempe\", \"Scottsdale\"),\n","  Row(\"116\", \"212\", \"312\", \"412\", \"20220405\", \"Washington\", \"Atlanta\"),\n","  Row(\"117\", \"212\", \"312\", \"412\", \"20220405\", \"Seattle\", \"Portland\"),\n","  Row(\"118\", \"212\", \"312\", \"412\", \"20220405\", \"Miami\", \"Tampa\")\n",")\n","\n","// Write Trips to Parquet\n","val tripWriteDF = spark.createDataFrame(spark.sparkContext.parallelize(tripData),tripSchema)\n","tripWriteDF.write.mode(\"overwrite\").parquet(tripsParquetPath)\n","\n","val driverSchema = new StructType().add(\"driverId\", StringType).add(\"name\", StringType).add(\"license\",StringType).add(\"gender\",StringType).add(\"salary\",IntegerType)\n","\n","val driverData = Seq(\n","  Row(\"200\", \"Alice\", \"A224455\", \"Female\", 3000),\n","  Row(\"202\", \"Bryan\",\"B992244\",\"Male\",4000),\n","  Row(\"204\", \"Catherine\",\"C887733\",\"Female\",4000),\n","  Row(\"208\", \"Daryl\",\"D229988\",\"Male\",3000),\n","  Row(\"212\", \"Jenny\",\"J663300\",\"Female\", 5000)\n",")\n","// Write Driver to Parquet\n","val driverWriteDF = spark.createDataFrame(spark.sparkContext.parallelize(driverData),driverSchema)\n","driverWriteDF.write.mode(\"overwrite\").parquet(driverParquetPath)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"534b2333-eaf9-43f9-9085-0c7ed76fa580","showTitle":false,"title":""},"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["%python\n","# Let us read the data from Parquet files and view them as a Dataframe using Python and SQL now\n","\n","storageAccountName = \"<INSERT STORAGE ACCOUNT>\"\n","fileSystemName = \"<INSERT CONTAINER NAME>\"\n","commonPath = \"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net\"\n","\n","df = spark.read.load( commonPath + '/parquet/trips/*.parquet',  format='parquet')\n","df.printSchema()\n","\n","spark.sql(\"DROP DATABASE TripsDatabase\")\n","\n","spark.sql(\"CREATE DATABASE IF NOT EXISTS TripsDatabase\")\n","df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"TripsTable\")\n","\n","sqldf = spark.sql(\"SELECT * FROM TripsTable\") \n","display(sqldf)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"626065bb-f5c6-4be1-946e-a5dd16de5412","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"html","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["df = spark.read.load( commonPath + '/parquet/trips/*.parquet',  format='parquet')\n","spark.sql(\"CREATE DATABASE IF NOT EXISTS TripsDatabase\")\n","df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"TripsTable\")\n","sqldf = spark.sql(\"\"\"\n","   SELECT COUNT(*) AS Trips, \n","   startLocation AS Location \n","   FROM TripsTable \n","   GROUP BY startLocation \"\"\") \n","display(sqldf)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"eb8ecfbe-3a99-4fa9-8d5e-40e6e9527705","showTitle":false,"title":""}},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"ParquetWithSynSpark-C7","notebookOrigID":188113580888537,"widgets":{}},"kernelspec":{"display_name":"python","name":"synapse_pyspark"},"language_info":{"name":"python"},"save_output":true,"synapse_widget":{"state":{},"version":"0.1"}},"nbformat":4,"nbformat_minor":0}
