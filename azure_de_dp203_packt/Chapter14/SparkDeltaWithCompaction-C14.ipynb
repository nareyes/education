{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"15eb6d2d-22a7-47be-99bd-167d143c324e","showTitle":false,"title":""}},"source":["Use the following Azure Databricks storage setup block only if you are using Azure Databricks. You can refer to the instructions here to get started:\n","https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sp-access\n","\n","If you are using Synapse Spark and if your data is residing on the storage attached to the Synapse Spark workspace, you can skip the below storage setup section."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e49b87b8-31ca-4f7e-b6d8-cd5e4b223522","showTitle":false,"title":""}},"outputs":[],"source":["%scala\n","val storageAccountName = \"<INSERT STORAGE ACCOUNT>\"\n","val fileSystemName = \"<INSERT CONTAINER NAME>\"\n","\n","val commonPath = \"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net\"\n","\n","# AAD Application Details\n","val appID = \"<INSERT APP ID>\"\n","val secret = \"<INSERT SECRET>\"\n","val tenantID = \"<INSERT TENANT ID>\"\n","\n","spark.conf.set(\"fs.azure.account.auth.type.\" + storageAccountName + \".dfs.core.windows.net\", \"OAuth\")\n","spark.conf.set(\"fs.azure.account.oauth.provider.type.\" + storageAccountName + \".dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.id.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + appID + \"\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.secret.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + secret + \"\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.\" + storageAccountName + \".dfs.core.windows.net\", \"https://login.microsoftonline.com/\" + tenantID + \"/oauth2/token\")\n","spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"true\")\n","dbutils.fs.ls(\"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net/\")\n","spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"06320451-6698-43af-857a-de0f29f050a4","showTitle":false,"title":""}},"outputs":[],"source":["%scala\n","import org.apache.spark.sql.{DataFrame, Row, SaveMode}\n","import org.apache.spark.sql.types.{StringType, IntegerType, StructField, StructType}\n","\n","val driverDeltaPath = commonPath + \"/delta\"\n","\n","val driverSchema = new StructType().add(\"driverID\", StringType).add(\"name\", StringType).add(\"license\",StringType).add(\"gender\",StringType).add(\"salary\",IntegerType)\n","\n","val driverData = Seq(\n","  Row(\"200\", \"Alice\", \"A224455\", \"Female\", 3000),\n","  Row(\"202\", \"Bryan\",\"B992244\",\"Male\",4000),\n","  Row(\"204\", \"Catherine\",\"C887733\",\"Female\",4000),\n","  Row(\"208\", \"Daryl\",\"D229988\",\"Male\",3000),\n","  Row(\"212\", \"Jenny\",\"J663300\",\"Female\", 5000)\n",")\n","\n","// Create a Dataframe using the above sample data\n","val driverWriteDF = spark.createDataFrame(spark.sparkContext.parallelize(driverData),driverSchema)\n","\n","// Write Driver to Delta\n","driverWriteDF.write.mode(\"overwrite\").format(\"delta\").save(driverDeltaPath)\n","\n","// Now let us read back from the delta location into a Dataframe\n","val driverDF: DataFrame = spark.read.format(\"delta\").load(driverDeltaPath)\n","\n","// Verify the data is available and correct\n","driverDF.show()\n","\n","spark.sql(\"CREATE TABLE IF NOT EXISTS Driver USING DELTA LOCATION '\" + driverDeltaPath + \"'\")\n","spark.sql(\"SELECT * FROM Driver\").show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5af49e39-5e5e-4759-aa21-9effe4881b9c","showTitle":false,"title":""},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["%scala\n","// Here is how you can compact the data using OPTMIZE command in Spark SQL\n","spark.sql(\"OPTIMIZE delta.`\" + commonPath + \"/delta`\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0cc9aed2-703c-4bb9-9651-247a6f6cd5f5","showTitle":false,"title":""}},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"SparkDeltaWithCompaction-C14","notebookOrigID":188113580888585,"widgets":{}},"kernelspec":{"display_name":"python","name":"synapse_pyspark"},"language_info":{"name":"python"},"save_output":true,"synapse_widget":{"state":{},"version":"0.1"}},"nbformat":4,"nbformat_minor":0}
