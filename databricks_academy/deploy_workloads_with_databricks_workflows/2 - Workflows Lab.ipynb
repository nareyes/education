{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4093658f-61a5-4fd5-b13e-e9e556cd3329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15181539-a02d-4c52-b1f0-fa920735ce8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Workflows Lab\n",
    "\n",
    "In this lab, you'll be configuring a multi-task job comprising of three notebooks.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you should be able to:\n",
    "* Schedule a notebook as a task in a Databricks Job\n",
    "* Configure linear dependencies between tasks using the Databricks Workflows UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af84c780-bcdd-4bc2-85b2-2e86a35dc52c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. It will also set your default catalog to **dbacademy** and the schema to your specific schema name shown below using the `USE` statements.\n",
    "<br></br>\n",
    "```\n",
    "USE CATALOG dbacademy;\n",
    "USE SCHEMA dbacademy.<your unique schema name>;\n",
    "```\n",
    "\n",
    "**NOTE:** The **DA** object is only used in Databricks Academy courses and is not available outside of these courses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36bd877c-834f-4a8c-9f6c-eae221a47707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "  - In the drop-down, select **More**.\n",
    "\n",
    "  - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a43d330-3824-445d-9ad4-e3590a29a995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table style=\"width:100%\">\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">Course Catalog:</td>\n",
       "                <td><input type=\"text\" value=\"dbacademy\" style=\"width: 100%\"></td></tr>\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">Your Schema:</td>\n",
       "                <td><input type=\"text\" value=\"labuser9084188_1738337208\" style=\"width: 100%\"></td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ./Includes/Classroom-Setup-2L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30f9c23b-8049-45e8-9726-893f0891e011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Generate Job Configuration\n",
    "1. Run the cell below to print out the values you'll use to configure your pipeline in subsequent steps. Make sure to specify the correct job name and notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49b087a6-3d65-4d50-8906-9b5ccb0c0042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table style=\"width:100%\">\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">Job Name:</td>\n",
       "                <td><input type=\"text\" value=\"labuser9084188_1738337208_Lesson_02\" style=\"width: 100%\"></td></tr>\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">Notebook #1:</td>\n",
       "                <td><input type=\"text\" value=\"/Workspace/Users/labuser9084188_1738337208@vocareum.com/deploy-workloads-with-databricks-workflows-2.0.1/Deploy Workloads with Databricks Workflows/Task Notebooks/Lesson 2 Notebooks/2.01 - Ingest CSV\" style=\"width: 100%\"></td></tr>\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">Notebook #2:</td>\n",
       "                <td><input type=\"text\" value=\"/Workspace/Users/labuser9084188_1738337208@vocareum.com/deploy-workloads-with-databricks-workflows-2.0.1/Deploy Workloads with Databricks Workflows/Task Notebooks/Lesson 2 Notebooks/2.02 - Create Invalid Region Table\" style=\"width: 100%\"></td></tr>\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">Notebook #3:</td>\n",
       "                <td><input type=\"text\" value=\"/Workspace/Users/labuser9084188_1738337208@vocareum.com/deploy-workloads-with-databricks-workflows-2.0.1/Deploy Workloads with Databricks Workflows/Task Notebooks/Lesson 2 Notebooks/2.02 - Create Valid Region Table\" style=\"width: 100%\"></td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DA.print_job_config(\n",
    "    job_name_extension='Lesson_02',\n",
    "    notebook_paths='/Task Notebooks/Lesson 2 Notebooks',\n",
    "    notebooks=[\n",
    "        '2.01 - Ingest CSV',\n",
    "        '2.02 - Create Invalid Region Table',\n",
    "        '2.02 - Create Valid Region Table'\n",
    "    ],\n",
    "    job_tasks={\n",
    "        'Ingest_CSV': [],\n",
    "        'Create_Invalid_Region_Table': ['Ingest_CSV'],\n",
    "        'Create_Valid_Region_Table': ['Ingest_CSV']\n",
    "    },\n",
    "    check_task_dependencies = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "384fd1fe-6680-4209-b130-8a9d53768807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Configure a Job With Multiple Tasks\n",
    "The job will complete three simple tasks:\n",
    "\n",
    "- (Notebook #1) Ingest a CSV file and create the **customers_bronze** table in your schema.\n",
    "- (Notebook #2) Create a table called **customers_invalid_region** in your schema.\n",
    "- (Notebook #3) Create a table called **customers_valid_region** in your schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5d66cec-045a-4150-a08c-5cdfde2746c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C1. Add a Single Notebook Task\n",
    "\n",
    "Let's start by scheduling the first notebook [2.01 - Ingest CSV]($./Task Notebooks/Lesson 2 Notebooks/2.01 - Ingest CSV) notebook. Click the hotlink in previous sentence to to review the code.\n",
    "\n",
    "The notebook creates a table named **customers_bronze** in your schema from the CSV file in the volume */Volumes/dbacademy_retail/v01/source_files/customers.csv*. \n",
    "\n",
    "1. Right click on the **Workflows** button on the sidebar and select *Open Link in New Tab*. \n",
    "\n",
    "2. In **Workflows** select the **Jobs** tab, and then click the **Create Job** button.\n",
    "\n",
    "3. In the top-left of the screen, enter the **Job Name** provided above to add a name for the job (must use the job name specified above).\n",
    "\n",
    "4. Configure the task as specified below. You'll need the values provided in the cell output above for this step.\n",
    "\n",
    "\n",
    "| Setting | Instructions |\n",
    "|--|--|\n",
    "| Task name | Enter **Ingest_CSV** |\n",
    "| Type | Choose **Notebook** |\n",
    "| Source | Choose **Workspace** |\n",
    "| Path | Use the navigator to specify the **Notebook #1** path provided above (notebook **Task Notebooks/Lesson 2 Notebooks/2.01 - Ingest CSV**) |\n",
    "| Compute | From the dropdown menu, select a **Serverless** cluster (We will be using Serverless clusters for jobs in this course. You can also specify a different cluster if required outside of this course) |\n",
    "\n",
    "**NOTE**: When selecting your all-purpose cluster, you may get a warning about how this will be billed as all-purpose compute. Production jobs should always be scheduled against new job clusters appropriately sized for the workload, as this is billed at a much lower rate.\n",
    "<br>\n",
    "\n",
    "![Lesson02_Lab_OneTask](files/images/deploy-workloads-with-databricks-workflows-2.0.1/Lesson02_Lab_OneTask.png)\n",
    "\n",
    "4. Click the **Create task** button.\n",
    "\n",
    "5. Click the blue **Run now** button in the top right to start the job.\n",
    "\n",
    "6. Select the **Runs** tab in the navigation bar and verify that the job completes successfully.\n",
    "\n",
    "![Lesson02_Lab_OneTaskSuccess](files/images/deploy-workloads-with-databricks-workflows-2.0.1/Lesson02_Lab_OneTaskSuccess.png)\n",
    "\n",
    "7. From **Catalog**, navigate to your schema in the **dbacademy** catalog and confirm the table **customers_bronze** was created (you might have refresh your schema)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50c58b0e-bf86-4c91-9d42-cc9785d7eea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C2. Add the Second Task to the Job\n",
    "\n",
    "Now, configure a second task that depends on the first task, **Ingest_CSV** successfully completing. The second task will be the notebook [2.02 - Create Invalid Table]($./Task Notebooks/Lesson 2 Notebooks/2.02 - Create Invalid Region Table). Open the notebook and review the code.\n",
    "\n",
    "The notebook creates a table named **customers_invalid_region** in your schema from the **customers_bronze** table created from the previous task.\n",
    "\n",
    "Steps:\n",
    "1. Go back to your job. On the Job details page, click the **Tasks** tab.\n",
    "\n",
    "2. Click the blue **+ Add task** button at the center bottom of the screen and select **Notebook** in the dropdown menu.\n",
    "\n",
    "3. Configure the task:\n",
    "\n",
    "| Setting | Instructions |\n",
    "|--|--|\n",
    "| Task name | Enter **Create_Invalid_Region_Table** |\n",
    "| Type | Choose **Notebook** |\n",
    "| Source | Choose **Workspace** |\n",
    "| Path | Use the navigator to specify the **Notebook #2** path provided above (notebook **Task Notebooks/Lesson 2 Notebooks/2.02 - Create Invalid Region Table**) |\n",
    "| Compute | From the dropdown menu, select a **Serverless** cluster (We will be using Serverless clusters for jobs in this course. You can also specify a different cluster if required outside of this course) |\n",
    "| Depends on | Verify **Ingest_CSV** (the previous task we defined) is listed |\n",
    "\n",
    "<br>\n",
    "\n",
    "4. Click the blue **Create task** button\n",
    "\n",
    "<br></br>\n",
    "\n",
    "![Lesson02_Lab_TwoTasks](files/images/deploy-workloads-with-databricks-workflows-2.0.1/Lesson02_Lab_TwoTasks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dab2c13-8c75-41e8-8d58-cb58b40d1921",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C3. Add the Third Task to the Job\n",
    "\n",
    "Now, configure a third task that depends on the **Ingest_CSV** successfully completing. The third task will be the notebook [2.03 - Create Valid Table]($./Task Notebooks/Lesson 2 Notebooks/2.02 - Create Valid Region Table). \n",
    "\n",
    "The notebook creates a table named **customers_valid_region** in your schema from the **customers_bronze** table created from the first task.\n",
    "\n",
    "Steps:\n",
    "1. On the Job details page, confirm you are on the **Tasks** tab.\n",
    "\n",
    "2. Click on the **Ingest_CSV** tasks.\n",
    "\n",
    "3. Click the blue **+ Add task** button at the center bottom of the screen and select **Notebook** in the dropdown menu.\n",
    "\n",
    "4. Configure the task:\n",
    "\n",
    "| Setting | Instructions |\n",
    "|--|--|\n",
    "| Task name | Enter **Create_Valid_Region_Table** |\n",
    "| Type | Choose **Notebook** |\n",
    "| Source | Choose **Workspace** |\n",
    "| Path | Use the navigator to specify the **Notebook #3** path provided above (notebook **Task Notebooks/Lesson 2 Notebooks/2.02 - Create Valid Region Table**) |\n",
    "| Compute | From the dropdown menu, select a **Serverless** cluster (We will be using Serverless clusters for jobs in this course. You can also specify a different cluster if required outside of this course) |\n",
    "| Depends on | Remove current **Depends on** task and replace with **Ingest_CSV** (the previous task we defined) is listed |\n",
    "\n",
    "5. Click the blue **Create task** button\n",
    "\n",
    "<br></br>\n",
    "\n",
    "![Lesson02_Lab_ThreeTasks](files/images/deploy-workloads-with-databricks-workflows-2.0.1/Lesson02_Lab_ThreeTasks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f85a352-af3e-4e78-a534-bcf42fd9d295",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Verify the Job is Configured Correctly\n",
    "Run the cell below to check if you configured the job correctly. Modify any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98e253f8-c1aa-4703-b934-5f73f8716403",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Required job Id has been found.\n2. Required job name labuser9084188_1738337208_Lesson_02 has been found.\n3. Required task notebooks set correctly.\n4. Job task names set correctly.\n5. Task dependencies are set correctly.\n-------------------------------------------\nYour Job has been validated. Tests passed!\n"
     ]
    }
   ],
   "source": [
    "DA.validate_job_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f287d02e-cd78-4b14-8a3d-a8c10adaef4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Run the Job\n",
    "1. Click the blue **Run now** button in the top right to run this job. It should take a few minutes to complete.\n",
    "\n",
    "2. From the **Runs** tab, you will be able to click on the start time for this run under the **Active runs** section and visually track task progress.\n",
    "\n",
    "3. On the **Runs** tab confirm that the job completed successfully.\n",
    "\n",
    "<br></br>\n",
    "![Lesson02_Lab_SuccessRun](files/images/deploy-workloads-with-databricks-workflows-2.0.1/Lesson02_Lab_SuccessRun.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "201d8f4a-3543-4fb0-922d-d0c0f42497f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## F. View the New Tables\n",
    "1. In the left pane, select **Catalog**.\n",
    "\n",
    "2. Expand the **dbacademy** catalog.\n",
    "\n",
    "3. Expand your unique schema name.\n",
    "\n",
    "4. Confirm that the job created the **customers_bronze**, **customers_invalid_region**, and **customers_valid_region** tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f334356d-1e10-4adc-879c-43a0fbb2014a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can also use the `SHOW TABLES` statement to view available tables in your schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42ffe06f-8d8c-4920-b341-3fd2fe48b9bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>tableName</th><th>isTemporary</th></tr></thead><tbody><tr><td>labuser9084188_1738337208</td><td>customers_bronze</td><td>false</td></tr><tr><td>labuser9084188_1738337208</td><td>customers_invalid_region</td><td>false</td></tr><tr><td>labuser9084188_1738337208</td><td>customers_valid_region</td><td>false</td></tr><tr><td>labuser9084188_1738337208</td><td>lesson1_workflow_users</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "labuser9084188_1738337208",
         "customers_bronze",
         false
        ],
        [
         "labuser9084188_1738337208",
         "customers_invalid_region",
         false
        ],
        [
         "labuser9084188_1738337208",
         "customers_valid_region",
         false
        ],
        [
         "labuser9084188_1738337208",
         "lesson1_workflow_users",
         false
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "database",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "tableName",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "isTemporary",
            "nullable": false,
            "type": "boolean"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 18
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e6dfc16-4111-4621-872a-96075ebbd30d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 553446609463683,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2L - Workflows Lab",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}