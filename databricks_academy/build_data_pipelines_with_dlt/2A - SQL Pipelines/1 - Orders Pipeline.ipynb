{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d07ebc7b-dc48-4ffd-9b63-bbdf1438c941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76270a8d-67fd-4b95-8ca4-220964407a23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Fundamentals of DLT SQL Syntax\n",
    "\n",
    "This notebook demonstrates using Delta Live Tables (DLT) to process raw data from JSON files landing in cloud object storage, through a series of tables, to drive analytic workloads in the lakehouse. Here we demonstrate a medallion architecture, where data is incrementally transformed and enriched as it flows through a pipeline. This notebook focuses on the SQL syntax of DLT rather than this architecture, but a brief overview of the design:\n",
    "\n",
    "* The bronze table contains raw records loaded from JSON enriched with data describing how records were ingested\n",
    "* The silver table validates and enriches the fields of interest\n",
    "* The gold table contains aggregate data to drive business insights and dashboarding\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, students should feel comfortable:\n",
    "* Declaring Delta Live Tables\n",
    "* Ingesting data with Auto Loader\n",
    "* Using parameters in DLT Pipelines\n",
    "* Enforcing data quality with constraints\n",
    "* Adding comments to tables\n",
    "* Describing differences in syntax and execution of Materialized View and streaming tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6131f57f-e0db-45af-8a43-b6fea7267181",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. About DLT Library Notebooks\n",
    "DLT syntax is not intended for interactive execution in a notebook, meaning you will not be able execute any cell below outside of a DLT pipeline. This notebook will need to be scheduled as part of a DLT pipeline for proper execution. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d86cfc1-5a98-4402-98e5-be23e5fc9060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Parameterization\n",
    "During the configuration of the DLT pipeline, a number of options were specified. One of these was a key-value pair added to the **Configuration** field with a key of **`source`**.\n",
    "\n",
    "Configurations in DLT pipelines are similar to parameters in Databricks Jobs or widgets in Databricks notebooks.\n",
    "\n",
    "Throughout these lessons, we'll be using the **`${source}`** parameter to perform string substitution of the file path set during configuration into our SQL queries. The path points to a Databricks volume.\n",
    "- **Example path:** /Volumes/dbacademy/ops/\\<your-unique-user-name>/stream-source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77e5f428-2645-45d6-8112-ba03b6671474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Tables as Query Results\n",
    "\n",
    "Delta Live Tables adapts standard SQL queries to combine DDL (data definition language) and DML (data manipulation language) into a unified declarative syntax.\n",
    "\n",
    "There are two distinct types of persistent tables that can be created with DLT. For both kinds of tables, DLT takes the approach of a slightly modified CTAS (create table as select) statement. Engineers just need to worry about writing queries to transform their data, and DLT handles the rest.\n",
    "\n",
    "- #### Materialized View  \n",
    "Materialized views are refreshed according to the update schedule of the pipeline in which theyâ€™re contained. Materialized views are powerful because they can handle any changes in the input. Each time the pipeline updates, query results are recalculated to reflect changes in upstream datasets that might have occurred because of compliance, corrections, aggregations, or general CDC.\n",
    "  <br></br>\n",
    "  **Syntax**\n",
    "  ```\n",
    "  CREATE OR REFRESH [MATERIALIZED VIEW] table_name\n",
    "  AS select_statement\n",
    "  ```\n",
    "\n",
    "  *Existing users of DLT will notice that the names have evolved. The previous syntax for a materialized view:*\n",
    "  ```\n",
    "  CREATE OR REFRESH [LIVE TABLE] table_name\n",
    "  AS select_statement\n",
    "  ```\n",
    "- #### Streaming Tables  \n",
    "Streaming tables allow you to process a growing dataset, handling each row only once. Because most datasets grow continuously over time, streaming tables are good for most ingestion workloads. Streaming tables are optimal for pipelines that require data freshness and low latency.\n",
    "  <br></br>\n",
    "  **Syntax**\n",
    "  ```\n",
    "  CREATE OR REFRESH [STREAMING TABLE] table_name\n",
    "  AS select_statement\n",
    "  ```\n",
    "\n",
    "  *Existing users of DLT will notice that the names have evolved. The previous syntax for a streaming table:*\n",
    "  ```\n",
    "  CREATE OR REFRESH [STREAMING LIVE TABLE] table_name\n",
    "  AS select_statement\n",
    "  ```\n",
    "  \n",
    "<br></br>\n",
    "Note that both of these objects are persisted as tables stored with the Delta Lake protocol (providing ACID transactions, versioning, and many other benefits). We'll talk more about the differences between materialized views and streaming tables later in the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d45ff96c-a7a0-48ca-baf2-cc7e34918fa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Streaming Ingestion with Auto Loader\n",
    "Databricks has developed the [Auto Loader](https://docs.databricks.com/ingestion/auto-loader/index.html) functionality to provide optimized execution for incrementally loading data from cloud object storage into Delta Lake. Using Auto Loader with DLT is simple: just configure a source data directory, provide a few configuration settings, and write a query against your source data. Auto Loader will automatically detect new data files as they land in the source cloud object storage location, incrementally processing new records without the need to perform expensive scans and recomputing results for infinitely growing datasets.\n",
    "\n",
    "The **`cloud_files()`** method enables Auto Loader to be used natively with SQL. This method takes the following positional parameters:\n",
    "* The source location, which should be cloud-based object storage\n",
    "* The source data format, which is JSON in this case\n",
    "* An arbitrarily sized comma-separated list of optional reader options. In this case, we set **`cloudFiles.inferColumnTypes`** to **`true`**\n",
    "\n",
    "In the query below, in addition to the fields contained in the source, Spark SQL functions for the **`current_timestamp()`** and **`_metadata.file_name`** as used to capture information about when the record was ingested and the specific file source for each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8882adb6-938d-41cf-a9f0-af836db58e46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH STREAMING TABLE orders_bronze\n",
    "AS \n",
    "SELECT \n",
    "  *,\n",
    "  current_timestamp() AS processing_time,\n",
    "  _metadata.file_name AS source_file\n",
    "FROM cloud_files(\"${source}/orders\", \"json\", map(\"cloudFiles.inferColumnTypes\", \"true\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "225777a6-ec4c-489a-9d6d-d8a9598772ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Validating, Enriching, and Transforming Data\n",
    "\n",
    "DLT allows users to easily declare tables from results of any standard Spark transformations. DLT leverages features used elsewhere in Spark SQL for documenting datasets, while adding new functionality for data quality checks.\n",
    "\n",
    "Let's break down the syntax of the query below.\n",
    "\n",
    "#### The Select Statement\n",
    "\n",
    "The select statement contains the core logic of your query. In this example, we:\n",
    "* Cast the field **`order_timestamp`** to the timestamp type\n",
    "* Select all of the remaining fields (except a list of 3 we're not interested in, including the original **`order_timestamp`**)\n",
    "\n",
    "Note that the **`FROM`** clause has two constructs that you may not be familiar with:\n",
    "* The **`LIVE`** keyword is used in place of the schema name to refer to the target schema configured for the current DLT pipeline\n",
    "* The **`STREAM`** method allows users to declare a streaming data source for SQL queries\n",
    "\n",
    "Note that if no target schema is declared during pipeline configuration, your tables won't be published (that is, they won't be registered to the metastore and made available for queries elsewhere). The target schema can be easily changed when moving between different execution environments, meaning the same code can easily be deployed against regional workloads or promoted from a dev to prod environment without needing to hard-code schema names.\n",
    "\n",
    "#### Data Quality Constraints\n",
    "\n",
    "DLT uses simple boolean statements to allow <a href=\"https://docs.databricks.com/delta-live-tables/expectations.html#delta-live-tables-data-quality-constraints&language-sql\" target=\"_blank\">quality enforcement checks</a> on data. In the statement below, we:\n",
    "* Declare a constraint named **`valid_date`**\n",
    "* Define the conditional check that the field **`order_timestamp`** must contain a value greater than January 1, 2021\n",
    "* Instruct DLT to fail the current transaction if any records violate the constraint\n",
    "\n",
    "Each constraint can have multiple conditions, and multiple constraints can be set for a single table. In addition to failing the update, constraint violation can also automatically drop records or just record the number of violations while still processing these invalid records.\n",
    "\n",
    "#### Table Comments\n",
    "\n",
    "Table comments are standard in SQL, and can be used to provide useful information to users throughout your organization. In this example, we write a short human-readable description of the table that describes how data is being ingested and enforced (which could also be gleaned from reviewing other table metadata).\n",
    "\n",
    "#### Table Properties\n",
    "\n",
    "The **`TBLPROPERTIES`** field can be used to pass any number of key/value pairs for custom tagging of data. Here, we set the value **`silver`** for the key **`quality`**.\n",
    "\n",
    "Note that while this field allows for custom tags to be arbitrarily set, it is also used for configuring number of settings that control how a table will perform. While reviewing table details, you may also encounter a number of settings that are turned on by default any time a table is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40f698ff-1e07-4c23-8716-881165b010e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH STREAMING TABLE orders_silver\n",
    "  (CONSTRAINT valid_date EXPECT (order_timestamp > \"2021-01-01\") ON VIOLATION FAIL UPDATE)\n",
    "COMMENT \"Append only orders with valid timestamps\"\n",
    "TBLPROPERTIES (\"quality\" = \"silver\")\n",
    "AS \n",
    "SELECT \n",
    "  timestamp(order_timestamp) AS order_timestamp, \n",
    "  * EXCEPT (order_timestamp, _rescued_data)\n",
    "FROM STREAM(LIVE.orders_bronze)                    -- References the orders_bronze streaming table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9d2d4cc-a80f-4dfa-aed5-932bb0d7bc9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## F. Materialized View vs. Streaming Tables\n",
    "\n",
    "The two queries we've reviewed so far have both created streaming tables. Below, we see a simple query that returns a live table (or materialized view) of some aggregated data.\n",
    "\n",
    "Spark has historically differentiated between batch queries and streaming queries. Live tables and streaming tables have similar differences.\n",
    "\n",
    "Note the only syntactic differences between streaming tables and live tables are the lack of the **`STREAMING`** keyword in the create clause and not wrapping the source table in the **`STREAM()`** method.\n",
    "\n",
    "Below are some of the differences between these types of tables.\n",
    "\n",
    "#### Materialized Views (also known as Live Tables)\n",
    "* Always \"correct\", meaning their contents will match their definition after any update.\n",
    "* Return same results as if table had just been defined for first time on all data.\n",
    "* Should not be modified by operations external to the DLT Pipeline (you'll either get undefined answers or your change will just be undone).\n",
    "\n",
    "#### Streaming Tables\n",
    "* Only supports reading from \"append-only\" streaming sources.\n",
    "* Only reads each input batch once, no matter what (even if joined dimensions change, or if the query definition changes, etc).\n",
    "* Can perform operations on the table outside the managed DLT Pipeline (append data, perform GDPR, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef5a4977-c37e-4f15-a808-02e59792e4d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH MATERIALIZED VIEW orders_by_date      -- PREVIOUS SYNTAX: CREATE OR REFRESH LIVE TABLE...\n",
    "AS \n",
    "SELECT \n",
    "  date(order_timestamp) AS order_date, \n",
    "  count(*) AS total_daily_orders\n",
    "FROM LIVE.orders_silver                                 -- References the full orders_silver streaming table\n",
    "GROUP BY date(order_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eea4f6ef-40c4-4166-89b8-893638b8ca7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "By reviewing this notebook, you should now feel comfortable:\n",
    "* Declaring Delta Live Tables\n",
    "* Ingesting data with Auto Loader\n",
    "* Using parameters in DLT Pipelines\n",
    "* Enforcing data quality with constraints\n",
    "* Adding comments to tables\n",
    "* Describing differences in syntax and execution of Materialized View and streaming tables\n",
    "\n",
    "In the next notebook, we'll continue learning about these syntactic constructs while adding a few new concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31d0837b-ce7e-4291-b7dc-24481b50afd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "sql",
   "notebookMetadata": {},
   "notebookName": "1 - Orders Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}