{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0f5a0d0-62c7-4416-a132-5405b6a2106b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1f2f3a0-9937-4b00-bd3b-7373184effb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# More DLT SQL Syntax\n",
    "\n",
    "DLT Pipelines make it easy to combine multiple datasets into a single scalable workload using one or many notebooks.\n",
    "\n",
    "In the last notebook, we reviewed some of the basic functionality of DLT syntax while processing data from cloud object storage through a series of queries to validate and enrich records at each step. This notebook similarly follows the medallion architecture, but introduces a number of new concepts.\n",
    "* Raw records represent change data capture (CDC) information about customers \n",
    "* The bronze table again uses Auto Loader to ingest JSON data from cloud object storage\n",
    "* A table is defined to enforce constraints before passing records to the silver layer\n",
    "* **`APPLY CHANGES INTO`** is used to automatically process CDC data into the silver layer as a Type 1 <a href=\"https://en.wikipedia.org/wiki/Slowly_changing_dimension\" target=\"_blank\">slowly changing dimension (SCD) table<a/>\n",
    "* A gold table is defined to calculate an aggregate from the current version of this Type 1 table\n",
    "* A view is defined that joins with tables defined in another notebook\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, students should feel comfortable:\n",
    "* Processing CDC data with **`APPLY CHANGES INTO`**\n",
    "* Declaring live views\n",
    "* Joining live tables\n",
    "* Describing how DLT library notebooks work together in a pipeline\n",
    "* Scheduling multiple notebooks in a DLT pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38d9c35f-dde1-4f6e-b798-d04e45ca380d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Ingest Data with Auto Loader\n",
    "\n",
    "As in the last notebook, we define a bronze table against a data source configured with Auto Loader.\n",
    "\n",
    "Note that the code below omits the Auto Loader option to infer schema. When data is ingested from JSON without the schema provided or inferred, fields will have the correct names but will all be stored as **`STRING`** type.\n",
    "\n",
    "The code below also provides a simple comment and adds fields for time of data ingestion and the file name for each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "776ccb87-b973-4fa6-a2d5-99754b6a6a06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH STREAMING TABLE customers_bronze\n",
    "  COMMENT \"Raw data from customers CDC feed\"\n",
    "AS \n",
    "SELECT \n",
    "  current_timestamp() processing_time, \n",
    "  *\n",
    "FROM cloud_files(\"${source}/customers\", \"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c97ff63a-a388-42af-96db-2aa3b8f06a1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Quality Enforcement Continued\n",
    "\n",
    "The query below demonstrates:\n",
    "* The 3 options for behavior when constraints are violated\n",
    "* A query with multiple constraints\n",
    "* Multiple conditions provided to one constraint\n",
    "* Using a built-in SQL function in a constraint\n",
    "\n",
    "About the data source:\n",
    "* Data is a CDC feed that contains **`INSERT`**, **`UPDATE`**, and **`DELETE`** operations. \n",
    "* Update and insert operations should contain valid entries for all fields.\n",
    "* Delete operations should contain **`NULL`** values for all fields other than the timestamp, **`customer_id`**, and operation fields.\n",
    "\n",
    "In order to ensure only good data makes it into our silver table, we'll write a series of quality enforcement rules that ignore the expected null values in delete operations.\n",
    "\n",
    "### We'll break down each of these constraints below:\n",
    "\n",
    "##### 1. **`valid_id`**\n",
    "This constraint will cause our transaction to fail if a record contains a null value in the **`customer_id`** field.\n",
    "\n",
    "##### 2. **`valid_operation`**\n",
    "This constraint will drop any records that contain a null value in the **`operation`** field.\n",
    "\n",
    "##### 3. **`valid_name`**\n",
    "This constraint will track any records that contain a null value in the **`name`** field. Because there is no additional instruction for what to do with invalid records, violating rows will be recorded in metrics but not dropped.\n",
    "\n",
    "##### 4. **`valid_address`**\n",
    "This constraint checks if the **`operation`** field is **`DELETE`**; if not, it checks for null values in any of the 4 fields comprising an address. Because there is no additional instruction for what to do with invalid records, violating rows will be recorded in metrics but not dropped.\n",
    "\n",
    "##### 5. **`valid_email`**\n",
    "This constraint uses regex pattern matching to check that the value in the **`email`** field is a valid email address. It contains logic to not apply this to records if the **`operation`** field is **`DELETE`** (because these will have a null value for the **`email`** field). Violating records are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "797e179a-6310-4605-afbd-f046d1d891ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE STREAMING TABLE customers_bronze_clean\n",
    "  (\n",
    "    CONSTRAINT valid_id EXPECT (customer_id IS NOT NULL) ON VIOLATION FAIL UPDATE,\n",
    "    CONSTRAINT valid_operation EXPECT (operation IS NOT NULL) ON VIOLATION DROP ROW,\n",
    "    CONSTRAINT valid_name EXPECT (name IS NOT NULL or operation = \"DELETE\"),\n",
    "    CONSTRAINT valid_address EXPECT (\n",
    "      (address IS NOT NULL and \n",
    "       city IS NOT NULL and \n",
    "       state IS NOT NULL and \n",
    "       zip_code IS NOT NULL) or\n",
    "       operation = \"DELETE\"),\n",
    "    CONSTRAINT valid_email EXPECT (\n",
    "      rlike(email, '^([a-zA-Z0-9_\\\\-\\\\.]+)@([a-zA-Z0-9_\\\\-\\\\.]+)\\\\.([a-zA-Z]{2,5})$') or \n",
    "      operation = \"DELETE\") ON VIOLATION DROP ROW\n",
    "  )\n",
    "AS \n",
    "SELECT *\n",
    "FROM STREAM(LIVE.customers_bronze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3705191-9549-4122-8dc7-d388cde772db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Processing CDC Data with **`APPLY CHANGES INTO`**\n",
    "DLT introduces a new syntactic structure for simplifying CDC feed processing.\n",
    "\n",
    "**`APPLY CHANGES INTO`** has the following guarantees and requirements:\n",
    "* Performs incremental/streaming ingestion of CDC data\n",
    "* Provides simple syntax to specify one or many fields as the primary key for a table\n",
    "* Default assumption is that rows will contain inserts and updates\n",
    "* Can optionally apply deletes\n",
    "* Automatically orders late-arriving records using user-provided sequencing key\n",
    "* Uses a simple syntax for specifying columns to ignore with the **`EXCEPT`** keyword\n",
    "* Will default to applying changes as Type 1 SCD\n",
    "\n",
    "The code below:\n",
    "* Creates the **`customers_silver`** table; **`APPLY CHANGES INTO`** requires the target table to be declared in a separate statement\n",
    "* Identifies the **`customers_silver`** table as the target into which the changes will be applied\n",
    "* Specifies the table **`customers_bronze_clean`** as the streaming source\n",
    "* Identifies the **`customer_id`** as the primary key\n",
    "* Specifies that records where the **`operation`** field is **`DELETE`** should be applied as deletes\n",
    "* Specifies the **`timestamp`** field for ordering how operations should be applied\n",
    "* Indicates that all fields should be added to the target table except **`operation`**, **`source_file`**, and **`_rescued_data`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c57f2444-5e71-44bb-aa46-a2f8b4e34e16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH STREAMING TABLE customers_silver;\n",
    "\n",
    "APPLY CHANGES INTO LIVE.customers_silver\n",
    "  FROM STREAM(LIVE.customers_bronze_clean)\n",
    "  KEYS (customer_id)\n",
    "  APPLY AS DELETE WHEN operation = \"DELETE\"\n",
    "  SEQUENCE BY timestamp\n",
    "  COLUMNS * EXCEPT (operation, _rescued_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4775df2f-b6f7-495d-959d-420c5d6004cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Querying Tables with Applied Changes\n",
    "\n",
    "**`APPLY CHANGES INTO`** defaults to creating a Type 1 SCD table, meaning that each unique key will have at most 1 record and that updates will overwrite the original information.\n",
    "\n",
    "While the target of our operation in the previous cell was defined as a streaming table, data is being updated and deleted in this table (and so breaks the append-only requirements for streaming table sources). As such, downstream operations cannot perform streaming queries against this table. \n",
    "\n",
    "This pattern ensures that if any updates arrive out of order, downstream results can be properly recomputed to reflect updates. It also ensures that when records are deleted from a source table, these values are no longer reflected in tables later in the pipeline.\n",
    "\n",
    "Below, we define a simple aggregate query to create a live table from the data in the **`customers_silver`** table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20059ee2-5f03-4466-b584-0aca59b73c04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE MATERIALIZED VIEW customer_counts_state           -- PREVIOUS SYNTAX: CREATE OR REFRESH LIVE TABLE...\n",
    "COMMENT \"Total active customers per state\"\n",
    "AS \n",
    "SELECT \n",
    "  state, \n",
    "  count(*) as customer_count, \n",
    "  current_timestamp() updated_at\n",
    "FROM LIVE.customers_silver\n",
    "GROUP BY state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fc1e578-5333-4484-9b1d-88e53de94a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. DLT Views\n",
    "\n",
    "The query below defines a DLT view by replacing **`TABLE`** with the **`VIEW`** keyword.\n",
    "\n",
    "Views in DLT differ from persisted tables, and can optionally be defined as **`STREAMING`**.\n",
    "\n",
    "Views have the same update guarantees as live tables, but the results of queries are not stored to disk.\n",
    "\n",
    "Unlike views used elsewhere in Databricks, DLT views are not persisted to the metastore, meaning that they can only be referenced from within the DLT pipeline they are a part of. (This is similar scoping to temporary views in most SQL systems.)\n",
    "\n",
    "Views can still be used to enforce data quality, and metrics for views will be collected and reported as they would be for tables.\n",
    "\n",
    "## Joins and Referencing Tables Across Notebook Libraries\n",
    "\n",
    "The code we've reviewed thus far has shown 2 source datasets propagating through a series of steps in separate notebooks.\n",
    "\n",
    "DLT supports scheduling multiple notebooks as part of a single DLT Pipeline configuration. You can edit existing DLT pipelines to add additional notebooks.\n",
    "\n",
    "Within a DLT Pipeline, code in any notebook library can reference tables and views created in any other notebook library.\n",
    "\n",
    "Essentially, we can think of the scope of the schema reference by the **`LIVE`** keyword to be at the DLT Pipeline level, rather than the individual notebook.\n",
    "\n",
    "In the query below, we create a new view by joining the silver tables from our **`orders`** and **`customers`** datasets. Note that this view is not defined as streaming; as such, we will always capture the current valid **`email`** for each customer, and will automatically drop records for customers after they've been deleted from the **`customers_silver`** table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a7deb2e-cd71-499a-95f8-58e17b6b7b6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE LIVE VIEW subscribed_order_emails_v         \n",
    "AS \n",
    "SELECT \n",
    "  a.customer_id, \n",
    "  a.order_id, \n",
    "  b.email \n",
    "FROM LIVE.orders_silver a\n",
    "INNER JOIN LIVE.customers_silver b\n",
    "ON a.customer_id = b.customer_id\n",
    "WHERE notifications = 'Y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b080436-1842-4455-9c7b-42be56efa7e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "By reviewing this notebook, you should now feel comfortable:\n",
    "* Processing CDC data with **`APPLY CHANGES INTO`**\n",
    "* Declaring live views\n",
    "* Joining live tables\n",
    "* Describing how DLT library notebooks work together in a pipeline\n",
    "* Scheduling multiple notebooks in a DLT pipeline\n",
    "\n",
    "In the next notebook, explore the output of our pipeline. Then we'll take a look at how to iteratively develop and troubleshoot DLT code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "740e67bc-0371-4248-a77d-58949371a6c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "sql",
   "notebookMetadata": {},
   "notebookName": "2 - Customers Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}