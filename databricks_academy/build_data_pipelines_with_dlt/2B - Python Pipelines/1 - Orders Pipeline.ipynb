{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8439545-fd15-4f22-845d-ef116bf4f12e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27462832-693b-4c66-ad81-01a6bc771e78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Fundamentals of DLT Python Syntax\n",
    "\n",
    "This notebook demonstrates using Delta Live Tables (DLT) to process raw data from JSON files landing in cloud object storage through a series of tables to drive analytic workloads in the lakehouse. Here we demonstrate a medallion architecture, where data is incrementally transformed and enriched as it flows through a pipeline. This notebook focuses on the Python syntax of DLT rather than this architecture, but a brief overview of the design:\n",
    "\n",
    "* The bronze table contains raw records loaded from JSON enriched with data describing how records were ingested\n",
    "* The silver table validates and enriches the fields of interest\n",
    "* The gold table contains aggregate data to drive business insights and dashboarding\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, students should feel comfortable:\n",
    "* Declaring Delta Live Tables\n",
    "* Ingesting data with Auto Loader\n",
    "* Using parameters in DLT Pipelines\n",
    "* Enforcing data quality with constraints\n",
    "* Adding comments to tables\n",
    "* Describing differences in syntax and execution of Materialized View and streaming tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93a87d77-a01e-4dfd-932f-899f99d61738",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. About DLT Library Notebooks\n",
    "\n",
    "DLT syntax is not intended for interactive execution in a notebook. This notebook will need to be scheduled as part of a DLT pipeline for proper execution. \n",
    "\n",
    "At the time this notebook was written, the current Databricks runtime does not include the **`dlt`** module, so trying to execute any DLT commands in a notebook will fail. \n",
    "\n",
    "We'll discuss developing and troubleshooting DLT code later in the course.\n",
    "\n",
    "#### Parameterization\n",
    "\n",
    "During the configuration of the DLT pipeline, a number of options were specified. One of these was a key-value pair added to the **Configurations** field.\n",
    "\n",
    "Configurations in DLT pipelines are similar to parameters in Databricks Jobs, but are actually set as Spark configurations.\n",
    "\n",
    "In Python, we can access these values using **`spark.conf.get()`**.\n",
    "\n",
    "Throughout these lessons, we'll set the Python variable **`source`** early in the notebook and then use this variable as necessary in the code. The path points to a Databricks volume.\n",
    "- **Example path:** /Volumes/dbacademy/ops/\\<your-unique-user-name>/stream-source\n",
    "\n",
    "#### A Note on Imports\n",
    "\n",
    "The **`dlt`** module should be explicitly imported into your Python notebook libraries.\n",
    "\n",
    "Here, we should importing **`pyspark.sql.functions`** as **`F`**.\n",
    "\n",
    "Some developers import **`*`**, while others will only import the functions they need in the present notebook.\n",
    "\n",
    "These lessons will use **`F`** throughout so that students clearly know which methods are imported from this library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1241ef85-2a96-4030-83a7-2e3339cfcb1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "source = spark.conf.get(\"source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97821ba9-7962-44f3-8564-f9c3efc7701a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Tables as DataFrames\n",
    "\n",
    "There are two distinct types of persistent tables that can be created with DLT:\n",
    "* **Materialized Views** are materialized views for the lakehouse; they will return the current results of any query with each refresh\n",
    "* **Streaming tables** are designed for incremental, near-real time data processing\n",
    "\n",
    "Note that both of these objects are persisted as tables stored with the Delta Lake protocol (providing ACID transactions, versioning, and many other benefits). We'll talk more about the differences between Materialized View and streaming tables later in the notebook.\n",
    "\n",
    "Delta Live Tables introduces a number of new Python functions that extend familiar PySpark APIs.\n",
    "\n",
    "At the heart of this design, the decorator **`@dlt.table`** is added to any Python function that returns a Spark DataFrame. (**NOTE**: This includes Koalas DataFrames, but these won't be covered in this course.)\n",
    "\n",
    "If you're used to working with Spark and/or Structured Streaming, you'll recognize the majority of the syntax used in DLT. The big difference is that you'll never see any methods or options for DataFrame writers, as this logic is handled by DLT.\n",
    "\n",
    "As such, the basic form of a DLT table definition will look like:\n",
    "\n",
    "**`@dlt.table`**<br/>\n",
    "**`def <function-name>():`**<br/>\n",
    "**`    return (<query>)`**</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba476007-eead-466b-a177-bf9faec16073",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## C. Streaming Ingestion with Auto Loader\n",
    "\n",
    "Databricks has developed the [Auto Loader](https://docs.databricks.com/ingestion/auto-loader/index.html) functionality to provide optimized execution for incrementally loading data from cloud object storage into Delta Lake. Using Auto Loader with DLT is simple: just configure a source data directory, provide a few configuration settings, and write a query against your source data. Auto Loader will automatically detect new data files as they land in the source cloud object storage location, incrementally processing new records without the need to perform expensive scans and recomputing results for infinitely growing datasets.\n",
    "\n",
    "Auto Loader can be combined with Structured Streaming APIs to perform incremental data ingestion throughout Databricks by configuring the **`format(\"cloudFiles\")`** setting. In DLT, you'll only configure settings associated with reading data, noting that the locations for schema inference and evolution will also be configured automatically if those settings are enabled.\n",
    "\n",
    "The query below returns a streaming DataFrame from a source configured with Auto Loader.\n",
    "\n",
    "In addition to passing **`cloudFiles`** as the format, here we specify:\n",
    "* The option **`cloudFiles.format`** as **`json`** (this indicates the format of the files in the cloud object storage location)\n",
    "* The option **`cloudFiles.inferColumnTypes`** as **`True`** (to auto-detect the types of each column)\n",
    "* The path of the cloud object storage to the **`load`** method\n",
    "* A select statement that includes a couple of **`pyspark.sql.functions`** to enrich the data alongside all the source fields\n",
    "\n",
    "By default, **`@dlt.table`** will use the name of the function as the name for the target table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eefd87e1-db8d-4455-9422-4fec00992a32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table\n",
    "def orders_bronze():\n",
    "    return (\n",
    "        spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"json\")\n",
    "            .option(\"cloudFiles.inferColumnTypes\", True)\n",
    "            .load(f\"{source}/orders\")\n",
    "            .select(\n",
    "                F.current_timestamp().alias(\"processing_time\"), \n",
    "                \"*\"\n",
    "            )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb837018-e27a-43b6-a9c7-b3cd678c96e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Validating, Enriching, and Transforming Data\n",
    "\n",
    "DLT allows users to easily declare tables from results of any standard Spark transformations. DLT adds new functionality for data quality checks and provides a number of options to allow users to enrich the metadata for created tables.\n",
    "\n",
    "Let's break down the syntax of the query below.\n",
    "\n",
    "### Options for **`@dlt.table()`**\n",
    "\n",
    "There are <a href=\"https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-python-ref.html#create-table\" target=\"_blank\">a number of options</a> that can be specified during table creation. Here, we use two of these to annotate our dataset.\n",
    "\n",
    "##### **`comment`**\n",
    "\n",
    "Table comments are a standard for relational databases. They can be used to provide useful information to users throughout your organization. In this example, we write a short human-readable description of the table that describes how data is being ingested and enforced (which could also be gleaned from reviewing other table metadata).\n",
    "\n",
    "##### **`table_properties`**\n",
    "\n",
    "This field can be used to pass any number of key/value pairs for custom tagging of data. Here, we set the value **`silver`** for the key **`quality`**.\n",
    "\n",
    "Note that while this field allows for custom tags to be arbitrarily set, it is also used for configuring number of settings that control how a table will perform. While reviewing table details, you may also encounter a number of settings that are turned on by default any time a table is created.\n",
    "\n",
    "### Data Quality Constraints\n",
    "\n",
    "The Python version of DLT uses decorator functions to set <a href=\"https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-expectations.html#delta-live-tables-data-quality-constraints\" target=\"_blank\">data quality constraints</a>. We'll see a number of these throughout the course.\n",
    "\n",
    "DLT uses simple boolean statements to allow quality enforcement checks on data. In the statement below, we:\n",
    "* Declare a constraint named **`valid_date`**\n",
    "* Define the conditional check that the field **`order_timestamp`** must contain a value greater than January 1, 2021\n",
    "* Instruct DLT to fail the current transaction if any records violate the constraint by using the decorator **`@dlt.expect_or_fail()`**\n",
    "\n",
    "Each constraint can have multiple conditions, and multiple constraints can be set for a single table. In addition to failing the update, constraint violation can also automatically drop records or just record the number of violations while still processing these invalid records.\n",
    "\n",
    "### DLT Read Methods\n",
    "\n",
    "The Python **`dlt`** module provides the **`read()`** and **`read_stream()`** methods to easily configure references to other tables and views in your DLT Pipeline. This syntax allows you to reference these datasets by name without any database reference. You can also use **`spark.table(\"LIVE.<table_name.\")`**, where **`LIVE`** is a keyword substituted for the database being referenced in the DLT Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c73fb52a-451a-4c09-82b2-eca1c6055b43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    comment = \"Append only orders with valid timestamps\",\n",
    "    table_properties = {\"quality\": \"silver\"})\n",
    "@dlt.expect_or_fail(\"valid_date\", F.col(\"order_timestamp\") > \"2021-01-01\")\n",
    "def orders_silver():\n",
    "    return (\n",
    "        dlt.read_stream(\"orders_bronze\")\n",
    "            .select(\n",
    "                \"processing_time\",\n",
    "                \"customer_id\",\n",
    "                \"notifications\",\n",
    "                \"order_id\",\n",
    "                F.col(\"order_timestamp\").cast(\"timestamp\").alias(\"order_timestamp\")\n",
    "            )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2653a3f7-5e5e-4aab-b333-d042a06c24d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Materialized View vs. Streaming Tables\n",
    "\n",
    "The two functions we've reviewed so far have both created streaming tables. Below, we see a simple function that returns a live table (or materialized view) of some aggregated data.\n",
    "\n",
    "Spark has historically differentiated between batch queries and streaming queries. Live tables and streaming tables have similar differences.\n",
    "\n",
    "Note that these table types inherit the syntax (as well as some of the limitations) of the PySpark and Structured Streaming APIs.\n",
    "\n",
    "Below are some of the differences between these types of tables.\n",
    "\n",
    "### Materialized View (also known as Live Tables)\n",
    "* Always \"correct\", meaning their contents will match their definition after any update.\n",
    "* Return same results as if table had just been defined for first time on all data.\n",
    "* Should not be modified by operations external to the DLT Pipeline (you'll either get undefined answers or your change will just be undone).\n",
    "\n",
    "### Streaming Tables\n",
    "* Only supports reading from \"append-only\" streaming sources.\n",
    "* Only reads each input batch once, no matter what (even if joined dimensions change, or if the query definition changes, etc).\n",
    "* Can perform operations on the table outside the managed DLT Pipeline (append data, perform GDPR, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5db7122-8c5a-4fdd-8f81-87694a2554e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table\n",
    "def orders_by_date():\n",
    "    return (\n",
    "        dlt.read(\"orders_silver\")\n",
    "            .groupBy(F.col(\"order_timestamp\").cast(\"date\").alias(\"order_date\"))\n",
    "            .agg(F.count(\"*\").alias(\"total_daily_orders\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfe76a56-429e-4420-9e1b-3a02fd63fe87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "By reviewing this notebook, you should now feel comfortable:\n",
    "* Declaring Delta Live Tables\n",
    "* Ingesting data with Auto Loader\n",
    "* Using parameters in DLT Pipelines\n",
    "* Enforcing data quality with constraints\n",
    "* Adding comments to tables\n",
    "* Describing differences in syntax and execution of live tables streaming tables\n",
    "\n",
    "In the next notebook, we'll continue learning about these syntactic constructs while adding a few new concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "570fb2fe-dc93-409e-a5e8-51b54bc56e8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "1 - Orders Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}