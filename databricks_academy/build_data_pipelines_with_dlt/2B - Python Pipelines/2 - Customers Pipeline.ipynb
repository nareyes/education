{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c718b384-be04-42f0-824d-bee27be7ae65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "536e6cdd-0f22-4de6-bc8d-ce5d9bedc314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# More DLT Python Syntax\n",
    "\n",
    "DLT Pipelines make it easy to combine multiple datasets into a single scalable workload using one or many notebooks.\n",
    "\n",
    "In the last notebook, we reviewed some of the basic functionalities of DLT syntax while processing data from cloud object storage through a series of queries to validate and enrich records at each step. This notebook similarly follows the medallion architecture, but introduces a number of new concepts.\n",
    "* Raw records represent change data capture (CDC) information about customers \n",
    "* The bronze table again uses Auto Loader to ingest JSON data from cloud object storage\n",
    "* A table is defined to enforce constraints before passing records to the silver layer\n",
    "* **`dlt.apply_changes()`** is used to automatically process CDC data into the silver layer as a Type 1 <a href=\"https://en.wikipedia.org/wiki/Slowly_changing_dimension\" target=\"_blank\">slowly changing dimension (SCD) table</a>\n",
    "* A gold table is defined to calculate an aggregate from the current version of this Type 1 table\n",
    "* A view is defined that joins with tables defined in another notebook\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, students should feel comfortable:\n",
    "* Processing CDC data with **`dlt.apply_changes()`**\n",
    "* Declaring live views\n",
    "* Joining live tables\n",
    "* Describing how DLT library notebooks work together in a pipeline\n",
    "* Scheduling multiple notebooks in a DLT pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cf5b79f-3bab-40fc-bf17-c22f098666ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "source = spark.conf.get(\"source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "463026cf-df3c-42a6-9001-9bc7bd48c7c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Ingest Data with Auto Loader\n",
    "\n",
    "As in the last notebook, we define a bronze table against a data source configured with Auto Loader.\n",
    "\n",
    "Note that the code below omits the Auto Loader option to infer schema. When data is ingested from JSON without the schema provided or inferred, fields will have the correct names but will all be stored as **`STRING`** type.\n",
    "\n",
    "#### Specifying Table Names\n",
    "\n",
    "The code below demonstrates the use of the **`name`** option for DLT table declaration. The option allows developers to specify the name for the resultant table separate from the function definition that creates the DataFrame the table is defined from.\n",
    "\n",
    "In the example below, we use this option to fulfill a table naming convention of **`<dataset-name>_<data-quality>`** and a function naming convention that describes what the function is doing. (If we hadn't specified this option, the table name would have been inferred from the function as **`ingest_customers_cdc`**.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c17450b5-c7d2-494a-8f8f-e67d0d731a46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name = \"customers_bronze\",\n",
    "    comment = \"Raw data from customers CDC feed\"\n",
    ")\n",
    "def ingest_customers_cdc():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .load(f\"{source}/customers\")\n",
    "        .select(\n",
    "            F.current_timestamp().alias(\"processing_time\"),\n",
    "            \"*\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cd8d72e-de14-4490-addc-dbdac0c17266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Quality Enforcement Continued\n",
    "\n",
    "The query below demonstrates:\n",
    "* The 3 options for behavior when constraints are violated\n",
    "* A query with multiple constraints\n",
    "* Multiple conditions provided to one constraint\n",
    "* Using a built-in SQL function in a constraint\n",
    "\n",
    "About the data source:\n",
    "* Data is a CDC feed that contains **`INSERT`**, **`UPDATE`**, and **`DELETE`** operations. \n",
    "* Update and insert operations should contain valid entries for all fields.\n",
    "* Delete operations should contain **`NULL`** values for all fields other than the timestamp, **`customer_id`**, and operation fields.\n",
    "\n",
    "In order to ensure only good data makes it into our silver table, we'll write a series of quality enforcement rules that ignore the expected null values in delete operations.\n",
    "\n",
    "We'll break down each of these constraints below:\n",
    "\n",
    "##### **`valid_id`**\n",
    "This constraint will cause our transaction to fail if a record contains a null value in the **`customer_id`** field.\n",
    "\n",
    "##### **`valid_operation`**\n",
    "This constraint will drop any records that contain a null value in the **`operation`** field.\n",
    "\n",
    "##### **`valid_address`**\n",
    "This constraint checks if the **`operation`** field is **`DELETE`**; if not, it checks for null values in any of the 4 fields comprising an address. Because there is no additional instruction for what to do with invalid records, violating rows will be recorded in metrics but not dropped.\n",
    "\n",
    "##### **`valid_email`**\n",
    "This constraint uses regex pattern matching to check that the value in the **`email`** field is a valid email address. It contains logic to not apply this to records if the **`operation`** field is **`DELETE`** (because these will have a null value for the **`email`** field). Violating records are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7da6aab-5fac-4e1c-9389-b6a824eb9543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table\n",
    "@dlt.expect_or_fail(\"valid_id\", \"customer_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_operation\", \"operation IS NOT NULL\")\n",
    "@dlt.expect(\"valid_name\", \"name IS NOT NULL or operation = 'DELETE'\")\n",
    "@dlt.expect(\"valid_adress\", \"\"\"\n",
    "    (address IS NOT NULL and \n",
    "    city IS NOT NULL and \n",
    "    state IS NOT NULL and \n",
    "    zip_code IS NOT NULL) or\n",
    "    operation = \"DELETE\"\n",
    "    \"\"\")\n",
    "@dlt.expect_or_drop(\"valid_email\", \"\"\"\n",
    "    rlike(email, '^([a-zA-Z0-9_\\\\\\\\-\\\\\\\\.]+)@([a-zA-Z0-9_\\\\\\\\-\\\\\\\\.]+)\\\\\\\\.([a-zA-Z]{2,5})$') or \n",
    "    operation = \"DELETE\"\n",
    "    \"\"\")\n",
    "def customers_bronze_clean():\n",
    "    return (\n",
    "        dlt.read_stream(\"customers_bronze\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57726c23-0713-4601-9f7c-60c8078bd975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Processing CDC Data with **`dlt.apply_changes()`**\n",
    "\n",
    "DLT introduces a new syntactic structure for simplifying CDC feed processing.\n",
    "\n",
    "**`dlt.apply_changes()`** has the following guarantees and requirements:\n",
    "* Performs incremental/streaming ingestion of CDC data\n",
    "* Provides simple syntax to specify one or many fields as the primary key for a table\n",
    "* Default assumption is that rows will contain inserts and updates\n",
    "* Can optionally apply deletes\n",
    "* Automatically orders late-arriving records using user-provided sequencing field\n",
    "* Uses a simple syntax for specifying columns to ignore with the **`except_column_list`**\n",
    "* Will default to applying changes as Type 1 SCD\n",
    "\n",
    "The code below:\n",
    "* Creates the **`customers_silver`** table; **`dlt.apply_changes()`** requires the target table to be declared in a separate statement\n",
    "* Identifies the **`customers_silver`** table as the target into which the changes will be applied\n",
    "* Specifies the table **`customers_bronze_clean`** as the source (**NOTE**: source must be append-only)\n",
    "* Identifies the **`customer_id`** as the primary key\n",
    "* Specifies the **`timestamp`** field for ordering how operations should be applied\n",
    "* Specifies that records where the **`operation`** field is **`DELETE`** should be applied as deletes\n",
    "* Indicates that all fields should be added to the target table except **`operation`**, and **`_rescued_data`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d60c6f1a-d30a-43c6-8be3-ff045b1b0aab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dlt.create_target_table(\n",
    "    name = \"customers_silver\")\n",
    "\n",
    "dlt.apply_changes(\n",
    "    target = \"customers_silver\",\n",
    "    source = \"customers_bronze_clean\",\n",
    "    keys = [\"customer_id\"],\n",
    "    sequence_by = F.col(\"timestamp\"),\n",
    "    apply_as_deletes = F.expr(\"operation = 'DELETE'\"),\n",
    "    except_column_list = [\"operation\", \"_rescued_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f596238-755d-4851-9f05-563b690ed330",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Querying Tables with Applied Changes\n",
    "\n",
    "**`dlt.apply_changes()`** defaults to creating a Type 1 SCD table, meaning that each unique key will have at most 1 record and that updates will overwrite the original information.\n",
    "\n",
    "While the target of our operation in the previous cell was defined as a streaming table, data is being updated and deleted in this table (and so breaks the append-only requirements for streaming table sources). As such, downstream operations cannot perform streaming queries against this table. \n",
    "\n",
    "This pattern ensures that if any updates arrive out of order, downstream results can be properly recomputed to reflect updates. It also ensures that when records are deleted from a source table, these values are no longer reflected in tables later in the pipeline.\n",
    "\n",
    "Below, we define a simple aggregate query to create a live table from the data in the **`customers_silver`** table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c72c043e-be03-4fb2-96bb-494cd37eca49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    comment=\"Total active customers per state\")\n",
    "def customer_counts_state():\n",
    "    return (\n",
    "        dlt.read(\"customers_silver\")\n",
    "            .groupBy(\"state\")\n",
    "            .agg( \n",
    "                F.count(\"*\").alias(\"customer_count\"), \n",
    "                F.first(F.current_timestamp()).alias(\"updated_at\")\n",
    "            )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f503e48f-503e-4fe6-94ba-0f94321307b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. DLT Views\n",
    "\n",
    "The query below defines a DLT view by using the **`@dlt.view`** decorator.\n",
    "\n",
    "Views in DLT differ from persisted tables, and can also inherit streaming execution from the function they decorate.\n",
    "\n",
    "Views have the same update guarantees as live tables, but the results of queries are not stored to disk.\n",
    "\n",
    "Unlike views used elsewhere in Databricks, DLT views are not persisted to the metastore, meaning that they can only be referenced from within the DLT pipeline they are a part of. (This is similar scoping to DataFrames in Databricks notebooks.)\n",
    "\n",
    "Views can still be used to enforce data quality, and metrics for views will be collected and reported as they would be for tables.\n",
    "\n",
    "### Joins and Referencing Tables Across Notebook Libraries\n",
    "\n",
    "The code we've reviewed thus far has shown 2 source datasets propagating through a series of steps in separate notebooks.\n",
    "\n",
    "DLT supports scheduling multiple notebooks as part of a single DLT Pipeline configuration. You can edit existing DLT pipelines to add additional notebooks.\n",
    "\n",
    "Within a DLT Pipeline, code in any notebook library can reference tables and views created in any other notebook library.\n",
    "\n",
    "Essentially, we can think of the scope of the database referenced by the **`LIVE`** keyword to be at the DLT Pipeline level, rather than the individual notebook.\n",
    "\n",
    "In the query below, we create a new view by joining the silver tables from our **`orders`** and **`customers`** datasets. Note that this view is not defined as streaming; as such, we will always capture the current valid **`email`** for each customer, and will automatically drop records for customers after they've been deleted from the **`customers_silver`** table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bae80ad-1eb4-41c6-84e9-88ced6a71036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.view\n",
    "def subscribed_order_emails_v():\n",
    "    return (\n",
    "        dlt.read(\"orders_silver\").filter(\"notifications = 'Y'\").alias(\"a\")\n",
    "            .join(\n",
    "                dlt.read(\"customers_silver\").alias(\"b\"), \n",
    "                on=\"customer_id\"\n",
    "            ).select(\n",
    "                \"a.customer_id\", \n",
    "                \"a.order_id\", \n",
    "                \"b.email\"\n",
    "            )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3d23ec5-d173-4f57-adb0-702ac2737bbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## F. Adding this Notebook to a DLT Pipeline\n",
    "\n",
    "Adding additional notebook libraries to an existing pipeline is accomplished easily with the DLT UI.\n",
    "\n",
    "1. Navigate to the DLT Pipeline you configured earlier in the course\n",
    "1. Click the **Settings** button in the top right\n",
    "1. Under **Notebook Libraries**, click **Add notebook library**\n",
    "   * Use the file picker to select this notebook, then click **Select**\n",
    "1. Click the **Save** button to save your updates\n",
    "1. Click the blue **Start** button in the top right of the screen to update your pipeline and process any new records\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_hint_24.png\"> The link to this notebook can be found back in [1b - Using the DLT UI - PART 2 - Customers]($../1b - Using the DLT UI - PART 2 - Customers)<br/>\n",
    "in the printed instructions for **Task #2** under the section **Generate Pipeline Configuration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b0e6063-0b4c-4049-9065-f0ad8c8f5fba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "By reviewing this notebook, you should now feel comfortable:\n",
    "* Processing CDC data with **`APPLY CHANGES INTO`**\n",
    "* Declaring live views\n",
    "* Joining live tables\n",
    "* Describing how DLT library notebooks work together in a pipeline\n",
    "* Scheduling multiple notebooks in a DLT pipeline\n",
    "\n",
    "In the next notebook, explore the output of our pipeline. Then we'll take a look at how to iteratively develop and troubleshoot DLT code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db024ffb-4858-41ac-ba94-acc4472b48be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2 - Customers Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}