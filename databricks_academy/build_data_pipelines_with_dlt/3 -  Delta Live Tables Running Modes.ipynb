{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7bb1b36-6eac-4a18-a3d5-9972b9c8688f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fc0e231-b8c3-4536-926f-fc0bd27fa4a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Delta Live Tables Running Modes\n",
    "We can trigger execution of DLT pipelines in two modes: triggered and continuous. In triggered mode, we can trigger the pipeline manually or schedule the pipeline to run on specific increments. Let's explore these modes more fully.\n",
    "\n",
    "Run the following setup script to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15a6144a-58e1-41dc-bfe7-22c4ac5ab591",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "  - In the drop-down, select **More**.\n",
    "\n",
    "  - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38b206c4-b340-4bfa-8598-c49b2c591855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. It will also set your default catalog to **dbacademy** and the schema to your specific schema name shown below using the `USE` statements.\n",
    "<br></br>\n",
    "\n",
    "\n",
    "```\n",
    "USE CATALOG dbacademy;\n",
    "USE SCHEMA dbacademy.<your unique schema name>;\n",
    "```\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63a7d817-4b97-4489-b9a7-24329411eaaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading batch 1 of 31...2 seconds\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run ./Includes/Classroom-Setup-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60d5622c-eaeb-447a-a2c1-19900f056cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**NOTE:** If you have not completed the DLT pipeline from the previous steps (**1a, 1b and 1c**), uncomment and run the following cell to create the pipeline using the solution SQL notebooks to complete this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4e362c0-80d3-4f92-b1aa-4090465da8b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created the DLT pipeline labuser9104086_1738770250: Example Pipeline using the settings from below:\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table style=\"width:100%\">\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">DLT Pipeline Name:</td>\n",
       "                <td><input type=\"text\" value=\"labuser9104086_1738770250: Example Pipeline\" style=\"width: 100%\"></td></tr>\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">Using Catalog:</td>\n",
       "                <td><input type=\"text\" value=\"dbacademy\" style=\"width: 100%\"></td></tr>\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">Using Schema:</td>\n",
       "                <td><input type=\"text\" value=\"labuser9104086_1738770250\" style=\"width: 100%\"></td></tr>\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">Compute:</td>\n",
       "                <td><input type=\"text\" value=\"Serverless\" style=\"width: 100%\"></td></tr>\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">Notebook #1:</td>\n",
       "                <td><input type=\"text\" value=\"/Workspace/Users/labuser9104086_1738770250@vocareum.com/build-data-pipelines-with-delta-live-tables-2.0.1/Build Data Pipelines with Delta Live Tables/2A - SQL Pipelines/(Solutions) 2A - SQL Pipelines/1 - Orders Pipeline\" style=\"width: 100%\"></td></tr>\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">Notebook #2:</td>\n",
       "                <td><input type=\"text\" value=\"/Workspace/Users/labuser9104086_1738770250@vocareum.com/build-data-pipelines-with-delta-live-tables-2.0.1/Build Data Pipelines with Delta Live Tables/2A - SQL Pipelines/(Solutions) 2A - SQL Pipelines/2 - Customers Pipeline\" style=\"width: 100%\"></td></tr>\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">Notebook #3:</td>\n",
       "                <td><input type=\"text\" value=\"/Workspace/Users/labuser9104086_1738770250@vocareum.com/build-data-pipelines-with-delta-live-tables-2.0.1/Build Data Pipelines with Delta Live Tables/2A - SQL Pipelines/(Solutions) 2A - SQL Pipelines/3L - Status Pipeline Lab\" style=\"width: 100%\"></td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DA.generate_pipeline(\n",
    "    pipeline_name=DA.generate_pipeline_name(), \n",
    "    use_schema = DA.schema_name,\n",
    "    notebooks_folder='2A - SQL Pipelines/(Solutions) 2A - SQL Pipelines', \n",
    "    pipeline_notebooks=[\n",
    "        '1 - Orders Pipeline',\n",
    "        '2 - Customers Pipeline',\n",
    "        '3L - Status Pipeline Lab'\n",
    "        ],\n",
    "    use_configuration = {'source':f'{DA.paths.stream_source}'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb07f7e3-b2b2-4053-8238-c50ca4b2a71d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the next cell. We are going to use its job name from the output in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ce18eef-783b-4820-83f7-46a33aa36ae8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labuser9104086_1738770250: Example Pipeline\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'labuser9104086_1738770250: Example Pipeline'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DA.print_pipeline_job_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d91cc88-69f3-466a-aaeb-b97840d7b419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Scheduled Execution\n",
    "DLT pipelines can be triggered on increments from one minute to one month, all at a specific time of day. Additionally, we can trigger alerts for when the pipeline starts, successfully completes, or fails. Follow the steps below to schedule DLT pipeline runs:\n",
    "\n",
    "1. Find your pipeline using the name from the above cell. Go to your pipeline's configuration page. \n",
    "\n",
    "2. In the upper-right corner, drop open the **`Schedule`**. Note the dialog that appears. We are actually going to be creating a job that will have one task, our pipeline\n",
    "\n",
    "3. In the **Advanced** tab, set the schedule to every month, but note the other options\n",
    "\n",
    "4. Select the **More options** drop down. Note that we can set notifications for start, success, and failure\n",
    "\n",
    "5. Click **Create**. A small window appears in the upper-right corner that shows our job was successfully created. The **Schedule** drop-down remains open and shows our current job, and we can add additional schedules, if needed.\n",
    "\n",
    "6. Select the job link in in the pop up. It will take you to the Workflow (job).\n",
    "\n",
    "7. Click the **Tasks** tab in the upper-left corner\n",
    "\n",
    "Our DLT pipeline is the only task for this job. If we wished, we could configure additional tasks.  \n",
    "  \n",
    "The task's configuration fields give use more options for the task.  We can: \n",
    "* Trigger a full refresh on the Delta Live Tables pipeline\n",
    "* Add, or change, notifications\n",
    "* Configure a retry policy for the task\n",
    "* Set duration thresholds where we can set times where we want to be warned that a pipeline is taking longer than expected or how long before we should cause the pipeline to timeout.  \n",
    "  \n",
    "8. Lastly, there are options on the right side for the whole job. We can see the schedule we created under **Schedules & Triggers**\n",
    "\n",
    "9. We will not be using the job. Click the \"kebab\" menu to the left of the **Run now** button, and select **Delete job**\n",
    "\n",
    "10. Leave the **Workflows** page open."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9edf03bf-bbaa-435a-b05a-6bddc2a88168",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Continuous Execution\n",
    "Setting a DLT pipeline to continuous execution will cause the pipeline to run continuously and process data as it arrives in our data sources. To avoid unnecessary processing in continuous execution mode, pipelines automatically monitor dependent Delta tables and perform an update only when the contents of those dependent tables have changed.\n",
    "\n",
    "Please note: After a pipeline is set to continuous mode and the pipeline is started, a cluster will run continuously until manually stopped. This will add to costs.\n",
    "\n",
    "To configure a DLT pipeline for continuous execution, complete the following:\n",
    "\n",
    "1. On the **Workflows** page select **Delta Live Tables** in the navigation bar and select your pipeline.\n",
    "\n",
    "2. On the pipeline configuration page, click **Settings** at the top right of the page.\n",
    "\n",
    "3. Under **Pipeline mode**, select **Continuous**\n",
    "\n",
    "4. Click **Save and start**\n",
    "\n",
    "The pipeline immediately begins its startup process. This process is very similar to a manually triggered start, except that after the first pipeline run, the pipeline will not shutdown, but will continue to monitor for new data.\n",
    "\n",
    "Wait for the pipeline to complete before starting the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fac7b44-a817-429f-a68e-ea3e52c9c115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Get Current Data from Silver Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b1b54f4-9455-4e4d-87f3-3cc6afdf37bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### C1. View the tables in your schema\n",
    "\n",
    "1. Navigate to the Catalog icon in the vertical bar (directly to the left of the notebook beneath the **File** icon). This will display all of the available catalogs to the left of the notebook. Do not select the **Catalog** text in the main navigation bar on the far left.\n",
    "\n",
    "2. Expand the **dbacademy** catalog.\n",
    "\n",
    "3. Expand your unique schema name.\n",
    "\n",
    "4. Notice that the DLT pipeline created all of the streaming tables and materialized views in the location you specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9731269-0f19-4c71-a8d5-559f9c2668f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C2. Query your materialized view\n",
    "Run the cell below to get the current data for a customer named \"Michael Lewis\".\n",
    "\n",
    "Note the current address for *Michael Lewis* is *706 Melissa Canyon*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76065eed-a9f4-4045-a2f0-b3c00aea1e1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>address</th></tr></thead><tbody><tr><td>Michael Lewis</td><td>706 Melissa Canyon</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Michael Lewis",
         "706 Melissa Canyon"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "address",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 25
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "address",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT name, address \n",
    "FROM customers_silver \n",
    "WHERE name = \"Michael Lewis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c9b50cd-e252-453c-98d1-edcec978d4e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Let's Add Data\n",
    "The `DA` object that was created in the Classroom-Setup script we ran at the beginning of this lesson contains a method that will add data to our volume. Let's use this method to see how our continuously running pipeline updates our resulting tables. \n",
    "\n",
    "We will be examining our pipeline results more fully in the next lesson. Run the next cell. While the cell is running you can watch your DLT pipeline and view as data is ingested and created.\n",
    "\n",
    "While the cell is running watch your DLT pipeline. Notice that the numbers are changing on each task.\n",
    "\n",
    "**Important** - The cell will throw an error if the pipeline has not completed at least one run. If you get an error that the table does not exist, wait for the pipeline to complete one run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22493361-1c98-494e-9a19-a5d9564c2b8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading batch 2 of 31...2 seconds\nLoading batch 3 of 31...1 seconds\nLoading batch 4 of 31...0 seconds\nLoading batch 5 of 31...1 seconds\nLoading batch 6 of 31...1 seconds\nLoading batch 7 of 31...1 seconds\nLoading batch 8 of 31...0 seconds\nLoading batch 9 of 31...1 seconds\nLoading batch 10 of 31...1 seconds\nLoading batch 11 of 31...1 seconds\nLoading batch 12 of 31...0 seconds\nLoading batch 13 of 31...1 seconds\nLoading batch 14 of 31...1 seconds\nLoading batch 15 of 31...0 seconds\nLoading batch 16 of 31...1 seconds\nLoading batch 17 of 31...0 seconds\nLoading batch 18 of 31...1 seconds\nLoading batch 19 of 31...1 seconds\nLoading batch 20 of 31...0 seconds\nLoading batch 21 of 31...1 seconds\nLoading batch 22 of 31...1 seconds\nLoading batch 23 of 31...1 seconds\nLoading batch 24 of 31...0 seconds\nLoading batch 25 of 31...1 seconds\nLoading batch 26 of 31...0 seconds\nLoading batch 27 of 31...1 seconds\nLoading batch 28 of 31...1 seconds\nLoading batch 29 of 31...0 seconds\nLoading batch 30 of 31...1 seconds\nLoading batch 31 of 31...0 seconds\nData source exhausted\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DA.dlt_data_factory.load(continuous=True, delay_seconds=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef7c01ef-a81d-4a81-9561-1ae4aa6cf1e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Requery Data\n",
    "Run the cell below, and note it is the same code that we ran before adding data to our source directory. \n",
    "\n",
    "We added 30 files of data to our source directory. This data included updated information for some of our fake customers. Michael Lewis changed his address, so this change should be reflected in our silver-level customers table. \n",
    "\n",
    "If you run this query too quickly, the pipeline will not have completed its update, and the address will not have changed. Wait a few seconds, and rerun the query.\n",
    "\n",
    "The results should show that *Michael Lewis* has an updated address to *451 Hunt Station*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93de5435-d9ea-400e-a6e8-1aad506b156d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>address</th></tr></thead><tbody><tr><td>Michael Lewis</td><td>451 Hunt Station</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Michael Lewis",
         "451 Hunt Station"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "address",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 32
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "address",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT name, address \n",
    "FROM customers_silver \n",
    "WHERE name = \"Michael Lewis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe1c987c-59a3-4683-b719-32a2b65753fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## F. (REQUIRED!) Stop the Pipeline\n",
    "If we do not stop the pipeline, it will continue to run indefinitely.\n",
    "\n",
    "* Stop the pipeline by clicking **`Stop`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84731bbd-56d3-41e3-971f-9cf907f76fd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2506747505182365,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3 -  Delta Live Tables Running Modes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}