{
    "metadata": {
        "kernelspec": {
            "name": "SQL",
            "display_name": "SQL",
            "language": "sql"
        },
        "language_info": {
            "name": "sql",
            "version": ""
        },
        "application/vnd.databricks.v1+notebook": {
            "dashboards": [],
            "language": "python",
            "notebookMetadata": {
                "pythonIndentUnit": 2
            },
            "notebookName": "SparkDeltaWithCompaction-C14",
            "notebookOrigID": 188113580888585,
            "widgets": {}
        },
        "save_output": true,
        "synapse_widget": {
            "state": {},
            "version": "0.1"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "Use the following Azure Databricks storage setup block only if you are using Azure Databricks. You can refer to the instructions here to get started:\n",
                "https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sp-access\n",
                "\n",
                "If you are using Synapse Spark and if your data is residing on the storage attached to the Synapse Spark workspace, you can skip the below storage setup section."
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "15eb6d2d-22a7-47be-99bd-167d143c324e",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "0eac1dca-7844-4766-9cd6-bdf58a9ee58a"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "%scala\n",
                "val storageAccountName = \"<INSERT STORAGE ACCOUNT>\"\n",
                "val fileSystemName = \"<INSERT CONTAINER NAME>\"\n",
                "\n",
                "val commonPath = \"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net\"\n",
                "\n",
                "# AAD Application Details\n",
                "val appID = \"<INSERT APP ID>\"\n",
                "val secret = \"<INSERT SECRET>\"\n",
                "val tenantID = \"<INSERT TENANT ID>\"\n",
                "\n",
                "spark.conf.set(\"fs.azure.account.auth.type.\" + storageAccountName + \".dfs.core.windows.net\", \"OAuth\")\n",
                "spark.conf.set(\"fs.azure.account.oauth.provider.type.\" + storageAccountName + \".dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
                "spark.conf.set(\"fs.azure.account.oauth2.client.id.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + appID + \"\")\n",
                "spark.conf.set(\"fs.azure.account.oauth2.client.secret.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + secret + \"\")\n",
                "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.\" + storageAccountName + \".dfs.core.windows.net\", \"https://login.microsoftonline.com/\" + tenantID + \"/oauth2/token\")\n",
                "spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"true\")\n",
                "dbutils.fs.ls(\"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net/\")\n",
                "spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "e49b87b8-31ca-4f7e-b6d8-cd5e4b223522",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "4d8614b0-5f03-4eeb-bb69-69b4807ca70e",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "%scala\n",
                "import org.apache.spark.sql.{DataFrame, Row, SaveMode}\n",
                "import org.apache.spark.sql.types.{StringType, IntegerType, StructField, StructType}\n",
                "\n",
                "val driverDeltaPath = commonPath + \"/delta\"\n",
                "\n",
                "val driverSchema = new StructType().add(\"driverID\", StringType).add(\"name\", StringType).add(\"license\",StringType).add(\"gender\",StringType).add(\"salary\",IntegerType)\n",
                "\n",
                "val driverData = Seq(\n",
                "  Row(\"200\", \"Alice\", \"A224455\", \"Female\", 3000),\n",
                "  Row(\"202\", \"Bryan\",\"B992244\",\"Male\",4000),\n",
                "  Row(\"204\", \"Catherine\",\"C887733\",\"Female\",4000),\n",
                "  Row(\"208\", \"Daryl\",\"D229988\",\"Male\",3000),\n",
                "  Row(\"212\", \"Jenny\",\"J663300\",\"Female\", 5000)\n",
                ")\n",
                "\n",
                "// Create a Dataframe using the above sample data\n",
                "val driverWriteDF = spark.createDataFrame(spark.sparkContext.parallelize(driverData),driverSchema)\n",
                "\n",
                "// Write Driver to Delta\n",
                "driverWriteDF.write.mode(\"overwrite\").format(\"delta\").save(driverDeltaPath)\n",
                "\n",
                "// Now let us read back from the delta location into a Dataframe\n",
                "val driverDF: DataFrame = spark.read.format(\"delta\").load(driverDeltaPath)\n",
                "\n",
                "// Verify the data is available and correct\n",
                "driverDF.show()\n",
                "\n",
                "spark.sql(\"CREATE TABLE IF NOT EXISTS Driver USING DELTA LOCATION '\" + driverDeltaPath + \"'\")\n",
                "spark.sql(\"SELECT * FROM Driver\").show()\n"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "06320451-6698-43af-857a-de0f29f050a4",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "d83e7309-5550-44da-89c6-2eca85428dab",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "%scala\n",
                "// Here is how you can compact the data using OPTMIZE command in Spark SQL\n",
                "spark.sql(\"OPTIMIZE delta.`\" + commonPath + \"/delta`\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "5af49e39-5e5e-4759-aa21-9effe4881b9c",
                    "showTitle": false,
                    "title": ""
                },
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                },
                "azdata_cell_guid": "2fb316bb-2caa-4258-b35d-26208b0c81df",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "%sql\r\n",
                "-- Optimize by Default\r\n",
                "CREATE TABLE Customer (\r\n",
                "    id INT\r\n",
                "    , name STRING\r\n",
                "    , location STRING\r\n",
                ")\r\n",
                "TBLPROPERTIES (\r\n",
                "    delta.autoOptimize.optimizeWrite = True\r\n",
                "    , delta.autoOptimize.optimizeCompact = True\r\n",
                ");"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "0cc9aed2-703c-4bb9-9651-247a6f6cd5f5",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "eb436eaf-9891-48a5-925c-234e488acb07",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "%scala \r\n",
                "# Write Delta\r\n",
                "df.write.mode(\"Overwrite\").format(\"delta).save(\"abfss://path/to/delta/files\")\r\n",
                "\r\n",
                "# Load Delta\r\n",
                "Val df: DataFrame = spark.read.format(\"delta\").load(\"abfss://path/to/delta/files\")\r\n",
                "\r\n",
                "# Create Delta\r\n",
                "Spark.sql(\"\"\"\r\n",
                "    CREATE TABLE CUSTOMER\r\n",
                "    USING DELTA\r\n",
                "    LOCATION \"abfss://path/to/delta/files\"\r\n",
                "\"\"\")"
            ],
            "metadata": {
                "azdata_cell_guid": "95979505-00d5-4334-b4c4-2f195db772cb",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}