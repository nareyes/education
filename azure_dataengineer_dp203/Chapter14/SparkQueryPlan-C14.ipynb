{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5ce0d083-c50c-4e45-8fc2-85dc286f4331","showTitle":false,"title":""}},"source":["Use the following Azure Databricks storage setup block only if you are using Azure Databricks. You can refer to the instructions here to get started:\n","https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sp-access\n","\n","If you are using Synapse Spark and if your data is residing on the storage attached to the Synapse Spark workspace, you can skip the below storage setup section."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"852dd2af-ff00-483c-9d0f-bd4f6d7a98fd","showTitle":false,"title":""}},"outputs":[],"source":["%scala\n","val storageAccountName = \"<INSERT STORAGE ACCOUNT>\"\n","val fileSystemName = \"<INSERT CONTAINER NAME>\"\n","\n","val commonPath = \"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net\"\n","\n","# AAD Application Details\n","val appID = \"<INSERT APP ID>\"\n","val secret = \"<INSERT SECRET>\"\n","val tenantID = \"<INSERT TENANT ID>\"\n","\n","spark.conf.set(\"fs.azure.account.auth.type.\" + storageAccountName + \".dfs.core.windows.net\", \"OAuth\")\n","spark.conf.set(\"fs.azure.account.oauth.provider.type.\" + storageAccountName + \".dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.id.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + appID + \"\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.secret.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + secret + \"\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.\" + storageAccountName + \".dfs.core.windows.net\", \"https://login.microsoftonline.com/\" + tenantID + \"/oauth2/token\")\n","spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"true\")\n","dbutils.fs.ls(\"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net/\")\n","spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5d61b9a4-591c-43bf-8d51-9c0a5423e2a7","showTitle":false,"title":""}},"outputs":[],"source":["%scala\n","// Let us run a simple join and then look at the query plan for it.\n","val jump2Numbers = spark.range(0, 100000,2) \n","val jump5Numbers = spark.range(0, 200000, 5) \n","val ds1 = jump2Numbers.repartition(3) \n","val ds2 = jump5Numbers.repartition(5) \n","val joined = ds1.join(ds2) \n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3767ab77-7f5e-4f8c-b216-31cd18f5a0fb","showTitle":false,"title":""}},"outputs":[],"source":["%scala\n","\n","// use the explain option to view the query plan\n","joined.explain"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"SparkQueryPlan-C14","notebookOrigID":188113580888591,"widgets":{}},"kernelspec":{"display_name":"python","name":"synapse_pyspark"},"language_info":{"name":"python"},"save_output":true,"synapse_widget":{"state":{},"version":"0.1"}},"nbformat":4,"nbformat_minor":0}
